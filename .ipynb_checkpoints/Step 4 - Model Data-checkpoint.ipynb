{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All libraries required to create a model capable of classifying tweets by category are imported.  These are described in the comments below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/isobeldaley/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "## Pandas required to manipulate data into user-friendly data structure\n",
    "import pandas as pd\n",
    "\n",
    "## Pickle allows Python objects to be saved for later use, and retrieved\n",
    "import pickle\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "## Numpy is used to execute various mathematical functions\n",
    "import numpy as np\n",
    "\n",
    "## Matplotlib and Seaborn are both plotting tools used to support datavisualisation\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "## Function to enable random split of data into training and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "## Import the TfidfVectorizer to convert a collection of raw documents to a matrix of TF-IDF features.\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "## Gridsearch enables the optimal combination of parameters to be selected for a given classifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "## A number of different classification models\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "import xgboost as xgb\n",
    "\n",
    "## Metrics to help evaluate the performance of each model\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Random Seed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure results are reproducible, a random see is set using Numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set a random seed to ensure results are reproducible\n",
    "np.random.seed(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Pandas Display Options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas display settings are chosen to ensure that the full contents of each column can be seen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set width of pandas dataframe to ensure entire Tweet is displayed\n",
    "pd.set_option('display.max_colwidth', 3000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cleaned and labelled tweets are imported from the pre-prepared DataFrame ('cleaned_labelled_tweets').  For more information on how this was created, please refer to **[Step 1 - Obtain Data](https://github.com/isobeldaley/categorising-tweets/blob/master/Step%201%20-%20Obtain%20Data.ipynb)** and **[Step 2 - Scrub Data](https://github.com/isobeldaley/categorising-tweets/blob/master/Step%202%20-%20Scrub%20Data.ipynb)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import saved dataframe using pickle\n",
    "df = pd.read_pickle('cleaned_labelled_tweets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the first five rows of the dataframe are previewed.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>network</th>\n",
       "      <th>datetime</th>\n",
       "      <th>original_tweet</th>\n",
       "      <th>subject</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>lemmatized_tweets_tokens</th>\n",
       "      <th>lemmatized_tweets_string</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@VodafoneUK</td>\n",
       "      <td>2019-12-04 08:05:14</td>\n",
       "      <td>@VodafoneUK Plus £2.28 package &amp;amp; posting ! ! !</td>\n",
       "      <td>device</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[plus, 2.28, package, posting]</td>\n",
       "      <td>plus 2.28 package posting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@VodafoneUK</td>\n",
       "      <td>2019-12-04 08:04:05</td>\n",
       "      <td>I have repeatedly asked how to get a refund so I can use another provider. I have also asked how to escalate my complaint. @VodafoneIN refuses to give me this information. @VodafoneUK @VodafoneGroup @rmstakkar @Nairkavita</td>\n",
       "      <td>customer service</td>\n",
       "      <td>-0.300000</td>\n",
       "      <td>[repeatedly, asked, get, refund, use, another, provider, also, asked, escalate, complaint, refuse, give, information]</td>\n",
       "      <td>repeatedly asked get refund use another provider also asked escalate complaint refuse give information</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@VodafoneUK</td>\n",
       "      <td>2019-12-04 08:01:19</td>\n",
       "      <td>I have supplied visa details twice, I have been subjected to horrendously rude staff instore, and now Vodafone are stealing my money by removing services I have paid for. Tourists should not use Vodafone. @VodafoneIn @VodafoneUK @VodafoneGroup @rmstakkar @Nairkavita</td>\n",
       "      <td>customer service</td>\n",
       "      <td>-0.300000</td>\n",
       "      <td>[supplied, visa, detail, twice, subjected, horrendously, rude, staff, instore, stealing, money, removing, service, paid, tourist, use]</td>\n",
       "      <td>supplied visa detail twice subjected horrendously rude staff instore stealing money removing service paid tourist use</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@VodafoneUK</td>\n",
       "      <td>2019-12-04 07:57:42</td>\n",
       "      <td>@VodafoneIN promised yesterday I’d receive no more calls and would get an email in 30 mins. No email received. Today I received yet another call. Vodaphone incompetence means I’ll be losing the data I’ve paid for from midnight @VodafoneUK @VodafoneGroup @rmstakkar @Nairkavita</td>\n",
       "      <td>customer service</td>\n",
       "      <td>-0.250000</td>\n",
       "      <td>[promised, yesterday, id, receive, call, would, get, email, 30, min, email, received, today, received, yet, another, call, vodaphone, incompetence, mean, ill, losing, data, ive, paid, midnight]</td>\n",
       "      <td>promised yesterday id receive call would get email 30 min email received today received yet another call vodaphone incompetence mean ill losing data ive paid midnight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@VodafoneUK</td>\n",
       "      <td>2019-12-04 07:57:16</td>\n",
       "      <td>@VodafoneUK you send texts about rewards - this morning Lindt. It takes me to my app but they are never there. Doesn’t matter how quickly I look. It actually becomes annoying.</td>\n",
       "      <td>promotion</td>\n",
       "      <td>-0.155556</td>\n",
       "      <td>[send, text, reward, morning, lindt, take, app, never, doesnt, matter, quickly, look, actually, becomes, annoying]</td>\n",
       "      <td>send text reward morning lindt take app never doesnt matter quickly look actually becomes annoying</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       network            datetime  \\\n",
       "0  @VodafoneUK 2019-12-04 08:05:14   \n",
       "1  @VodafoneUK 2019-12-04 08:04:05   \n",
       "2  @VodafoneUK 2019-12-04 08:01:19   \n",
       "3  @VodafoneUK 2019-12-04 07:57:42   \n",
       "4  @VodafoneUK 2019-12-04 07:57:16   \n",
       "\n",
       "                                                                                                                                                                                                                                                                         original_tweet  \\\n",
       "0                                                                                                                                                                                                                                    @VodafoneUK Plus £2.28 package &amp; posting ! ! !   \n",
       "1                                                         I have repeatedly asked how to get a refund so I can use another provider. I have also asked how to escalate my complaint. @VodafoneIN refuses to give me this information. @VodafoneUK @VodafoneGroup @rmstakkar @Nairkavita   \n",
       "2            I have supplied visa details twice, I have been subjected to horrendously rude staff instore, and now Vodafone are stealing my money by removing services I have paid for. Tourists should not use Vodafone. @VodafoneIn @VodafoneUK @VodafoneGroup @rmstakkar @Nairkavita   \n",
       "3  @VodafoneIN promised yesterday I’d receive no more calls and would get an email in 30 mins. No email received. Today I received yet another call. Vodaphone incompetence means I’ll be losing the data I’ve paid for from midnight @VodafoneUK @VodafoneGroup @rmstakkar @Nairkavita   \n",
       "4                                                                                                       @VodafoneUK you send texts about rewards - this morning Lindt. It takes me to my app but they are never there. Doesn’t matter how quickly I look. It actually becomes annoying.   \n",
       "\n",
       "            subject  sentiment  \\\n",
       "0            device   0.000000   \n",
       "1  customer service  -0.300000   \n",
       "2  customer service  -0.300000   \n",
       "3  customer service  -0.250000   \n",
       "4         promotion  -0.155556   \n",
       "\n",
       "                                                                                                                                                                            lemmatized_tweets_tokens  \\\n",
       "0                                                                                                                                                                     [plus, 2.28, package, posting]   \n",
       "1                                                                              [repeatedly, asked, get, refund, use, another, provider, also, asked, escalate, complaint, refuse, give, information]   \n",
       "2                                                             [supplied, visa, detail, twice, subjected, horrendously, rude, staff, instore, stealing, money, removing, service, paid, tourist, use]   \n",
       "3  [promised, yesterday, id, receive, call, would, get, email, 30, min, email, received, today, received, yet, another, call, vodaphone, incompetence, mean, ill, losing, data, ive, paid, midnight]   \n",
       "4                                                                                 [send, text, reward, morning, lindt, take, app, never, doesnt, matter, quickly, look, actually, becomes, annoying]   \n",
       "\n",
       "                                                                                                                                                 lemmatized_tweets_string  \n",
       "0                                                                                                                                               plus 2.28 package posting  \n",
       "1                                                                  repeatedly asked get refund use another provider also asked escalate complaint refuse give information  \n",
       "2                                                   supplied visa detail twice subjected horrendously rude staff instore stealing money removing service paid tourist use  \n",
       "3  promised yesterday id receive call would get email 30 min email received today received yet another call vodaphone incompetence mean ill losing data ive paid midnight  \n",
       "4                                                                      send text reward morning lindt take app never doesnt matter quickly look actually becomes annoying  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Preview first five rows of DataFrame\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen above, the tweets are categorised by network, subject and sentiment.  They are also stored in three different forms:\n",
    "\n",
    "- The raw/original tweet\n",
    "- The cleaned/lemmatized tweet as tokens\n",
    "- The cleaned/lemmatized tweet as a string\n",
    "\n",
    "This is to allow greatest flexibility when modelling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data into Training & Test Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure that the models created work well and do not sufer from overfitting, the dataset is split into a training and test set. The training set will be used to build the model, whilst the test set will be used to validate that it works well and can be generalised to new data.\n",
    "\n",
    "To do this, it is first necessary to define the independent (X) and dependent (y) variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the X and y variables\n",
    "X = df['lemmatized_tweets_string']\n",
    "y = df['subject']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **train_test_split()** function can then be used to create this split.  Note that a random_state is specified. This is to ensure results can be reproduced by others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Split dataset into training & test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2, random_state=213)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most machine learning algorithms are not able to process raw text directly.  As such, it is first necessary to convert the raw text into vectors of numbers.  \n",
    "\n",
    "There are a number of different ways of doing this.  This includes using a:\n",
    "\n",
    "- **Count Vectorizer**, which simply counts the number of times a word appears in a tweet and uses this as its weight.\n",
    "\n",
    "- **TF-IDF Vectorizer**, which evaulates how important a specified word is in a tweet.  This works by increasing the importance of a word in proportion to the number of times it appears in a particular tweet, but reducing the importance of that word by the frequency the word appears in the entire dataset of tweets.  In this way, it helps the algorithm determine which words are key to categorising a given tweet.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sklearn provides an inbuilt CountVectorizer().  For greatest efficiency, this is used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Specify the CountVectorizer, as provided by sklearn\n",
    "count_vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having specified the vectorizer, it is fitted using the training data.  The training data is then transformed.  It is important not to fit using the test data, as this may lead to data leakage from the training to the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fit and trasnform the training data using the tf-idf vectorizer\n",
    "count_X_train = count_vectorizer.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally the test data is also transformed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Transform the test data using this vectorizer\n",
    "count_X_test = count_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 789)\t1\n",
      "  (0, 1381)\t1\n",
      "  (0, 1437)\t1\n",
      "  (0, 2234)\t1\n",
      "  (0, 3753)\t1\n",
      "  (0, 4409)\t1\n",
      "  (0, 4434)\t1\n",
      "  (0, 5272)\t1\n"
     ]
    }
   ],
   "source": [
    "print(count_X_test[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF Vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sklearn provides an inbuilt TfidfVectorizer().  For greatest efficiency, this is used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Specify the tfidfvectorizer, as provided by sklearn\n",
    "tfidf_vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having specified the vectorizer, it is fitted using the training data.  The training data is then transformed.  It is important not to fit using the test data, as this may lead to data leakage from the training to the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fit and trasnform the training data using the tf-idf vectorizer\n",
    "tf_idf_X_train = tfidf_vectorizer.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the test data is also transformed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Transform the test data using this vectorizer\n",
    "tf_idf_X_test = tfidf_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before building and comparing alternative classification models, the following function has been defined. This function identifies the most effective combination of parameters and the best data transformation to enhance model performance for a given classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to identify the optimat dataset and parameters for a given classifier and parameter grid\n",
    "def best_model_parameters_dataset(classifier, param_grid, datasets):\n",
    "    \n",
    "    ## Create a list to contain the dataset, optimal parameters, and score for training and test set\n",
    "    score_parameters = []\n",
    "    \n",
    "    ## Create a for loop which iterates through each dataset and identifies the optimal parameters for the given classifier\n",
    "    for data in datasets:\n",
    "        \n",
    "        gs = GridSearchCV(classifier, param_grid, scoring='accuracy', cv=3)\n",
    "        gs.fit(data['X_train'], data['y_train'])\n",
    "        y_test_preds = gs.predict(data['X_test'])\n",
    "        test_score = accuracy_score(y_test_preds, data['y_test'])\n",
    "        score_parameters.append({'Dataset':data['name'], 'Training Score':round(gs.best_score_,2), 'Test Score': round(test_score,2), 'Parameters':gs.best_params_})\n",
    "     \n",
    "    ## Generate a dataframe that contains the optimal parameters for each dataset\n",
    "    df = pd.DataFrame(score_parameters)\n",
    "    df.sort_values(by=['Test Score', 'Training Score'], inplace=True, ascending=False)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A number of different classification models will be assessed for their suitability for the task of predicting the category a given tweet relates to.  These are:\n",
    "\n",
    "- K Nearest Neighbors\n",
    "- Naive Multinomial Bayes Classifier\n",
    "- Multinomial Logistic Regression\n",
    "- Random Forest Classifier\n",
    "- XG Boost\n",
    "- Support Vector Machine (SVM)\n",
    "\n",
    "Each model will be created using the two datasets specified above:\n",
    "\n",
    "- Dataset transformed by tf-idf vectorization\n",
    "- Dataset transformed by count vectorization\n",
    "\n",
    "To make this task more efficient, a list containing all training and test datasets is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a list of datasets.  Each item is a dictionary detailing training/test datasets\n",
    "datasets = [{'name': 'tf_idf','X_train': tf_idf_X_train, 'y_train':y_train, 'X_test': tf_idf_X_test, 'y_test':y_test},\n",
    "           {'name':'count','X_train':count_X_train,'y_train':y_train, 'X_test': count_X_test, 'y_test':y_test}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K Nearest Neighbours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The K Nearest Neighbours (KNN) works by identifying a specified number of similar observations ('nearest neighbours) based on a specified distance metric, and then providing a classification based on the majority classification of the identified 'neighbours'.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Specify the classifier, in this case K nearest neighbours\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "## Define the parameter grid\n",
    "knn_param_grid = {'n_neighbors':[5,20,40,50,60],\n",
    "              'metric': ['manhattan', 'euclidean','minkowski'],\n",
    "              'weights': ['uniform', 'distance']\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Parameters</th>\n",
       "      <th>Test Score</th>\n",
       "      <th>Training Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>count</td>\n",
       "      <td>{'metric': 'euclidean', 'n_neighbors': 5, 'weights': 'distance'}</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tf_idf</td>\n",
       "      <td>{'metric': 'manhattan', 'n_neighbors': 5, 'weights': 'distance'}</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.42</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Dataset                                                        Parameters  \\\n",
       "1   count  {'metric': 'euclidean', 'n_neighbors': 5, 'weights': 'distance'}   \n",
       "0  tf_idf  {'metric': 'manhattan', 'n_neighbors': 5, 'weights': 'distance'}   \n",
       "\n",
       "   Test Score  Training Score  \n",
       "1        0.52            0.49  \n",
       "0        0.44            0.42  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Run the best_model_parameters_dataset() function to identify the optimal dataset and parameters to use\n",
    "best_model_parameters_dataset(knn, knn_param_grid, datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From teh above, it appears that the best performance is achieved when the dataset that has been transformed by a CountVectorizer() is used, and the following parameters are specified:\n",
    "\n",
    "- **Distance Metric**: Euclidean\n",
    "- **Number of Neighbours**: 5\n",
    "- **Weights**: Distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To assess the performance of this model in morde detail, the model will be created using these optimal parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='euclidean',\n",
       "                     metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
       "                     weights='distance')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Create a KNN classifier using the optimal parameters specified above\n",
    "knn = KNeighborsClassifier(metric='euclidean', n_neighbors=5, weights='distance')\n",
    "\n",
    "## Fit the model using the count vectorized dataset\n",
    "knn.fit(count_X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, predictions can be generated using the model for both the test and training set.  These will be used to calculate a number of metrics to assess performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_preds_train = knn.predict(count_X_train)\n",
    "knn_preds_test = knn.predict(count_X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classification report provides the following key metrics:\n",
    "- **Precision**: Defined as the number of times the model correctly assigned a given classification,  as a proportion of all observations with that predicted classification.  A low precison suggests a high rate of false positives.  \n",
    "- **Recall**: Defined as the number of times the model correctly assigned a given classification,  as a proportion of all observations with that actually had that classification.  A low recall suggests a high rate of false negatives.  \n",
    "- **f1-score**: The harmonic mean of precision and recall.  \n",
    "- **Accuracy**: The proportion of all observations that were correctly classified\n",
    "\n",
    "Note that there is an inverse relationship between precision and recall.  Balance is therefore important.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  precision    recall  f1-score   support\n",
      "\n",
      "       broadband       1.00      1.00      1.00        74\n",
      "        contract       0.98      1.00      0.99       427\n",
      "customer service       0.99      0.99      0.99       753\n",
      "          device       1.00      1.00      1.00       217\n",
      "         network       1.00      1.00      1.00       436\n",
      "           other       0.99      0.99      0.99      1341\n",
      "       promotion       1.00      0.99      1.00       253\n",
      "\n",
      "        accuracy                           0.99      3501\n",
      "       macro avg       0.99      0.99      0.99      3501\n",
      "    weighted avg       0.99      0.99      0.99      3501\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Print the classification report for the training dataset\n",
    "print(classification_report(y_train, knn_preds_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  precision    recall  f1-score   support\n",
      "\n",
      "       broadband       0.50      0.10      0.17        10\n",
      "        contract       0.58      0.11      0.18       101\n",
      "customer service       0.64      0.35      0.45       178\n",
      "          device       0.48      0.20      0.29        59\n",
      "         network       0.73      0.18      0.29       106\n",
      "           other       0.49      0.95      0.65       354\n",
      "       promotion       0.71      0.22      0.34        68\n",
      "\n",
      "        accuracy                           0.52       876\n",
      "       macro avg       0.59      0.30      0.34       876\n",
      "    weighted avg       0.58      0.52      0.46       876\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Print the classification report for the test daaset\n",
    "print(classification_report(y_test, knn_preds_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following observations can be made:\n",
    "\n",
    "- There is a significant mismatch between the weighted average of accuracy of the training and test set (99% v. 52%).  This is indicative of overfitting, and suggests the model does not generalize well to new data.  \n",
    "\n",
    "- Whilst there is good balance between precision and recall for the training data, this is not the case for the test data.  With the exception of the 'other' category, precision is notably larger than recall.  This suggests that there is an issue with false negatives.\n",
    "\n",
    "The performance of this model is too low to take it forward to production.  Alternatives must be sought."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the Multinomial Naive Bayes Classifier is assessed for suitability.  This classifier is based on Bayes theorem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Specify the classifier, in this case naive bayes\n",
    "nb = MultinomialNB()\n",
    "\n",
    "## Create a parameter grid to identify optimal parameters\n",
    "nb_param_grid = {'alpha':[0.5,0.8,1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Parameters</th>\n",
       "      <th>Test Score</th>\n",
       "      <th>Training Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>count</td>\n",
       "      <td>{'alpha': 0.8}</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tf_idf</td>\n",
       "      <td>{'alpha': 0.5}</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.61</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Dataset      Parameters  Test Score  Training Score\n",
       "1   count  {'alpha': 0.8}        0.69            0.66\n",
       "0  tf_idf  {'alpha': 0.5}        0.64            0.61"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Run the best_model_parameters_dataset() function to identify the optimal dataset and parameters to use\n",
    "best_model_parameters_dataset(nb, nb_param_grid, datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To assess the performance further, this model will be created using the optimal combination of parameters:\n",
    "- Count Vectorized dataset\n",
    "- Alpha = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=0.8, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Create a multinomial NB classifier using the optimal parameters specified above\n",
    "mnb = MultinomialNB(alpha=0.8)\n",
    "\n",
    "## Fit the model using the count vectorized dataset\n",
    "mnb.fit(count_X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, predictions can be generated using the model for both the test and training set. These will be used to calculate a number of metrics to assess performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate predictions for the MNB model\n",
    "mnb_preds_train = mnb.predict(count_X_train)\n",
    "mnb_preds_test = mnb.predict(count_X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  precision    recall  f1-score   support\n",
      "\n",
      "       broadband       1.00      0.49      0.65        74\n",
      "        contract       0.84      0.86      0.85       427\n",
      "customer service       0.75      0.92      0.83       753\n",
      "          device       0.97      0.71      0.82       217\n",
      "         network       0.87      0.90      0.88       436\n",
      "           other       0.91      0.87      0.89      1341\n",
      "       promotion       0.95      0.79      0.86       253\n",
      "\n",
      "        accuracy                           0.86      3501\n",
      "       macro avg       0.90      0.79      0.83      3501\n",
      "    weighted avg       0.87      0.86      0.86      3501\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Print the classification report for the training dataset\n",
    "print(classification_report(y_train, mnb_preds_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  precision    recall  f1-score   support\n",
      "\n",
      "       broadband       1.00      0.10      0.18        10\n",
      "        contract       0.56      0.61      0.58       101\n",
      "customer service       0.56      0.81      0.66       178\n",
      "          device       0.71      0.34      0.46        59\n",
      "         network       0.73      0.77      0.75       106\n",
      "           other       0.81      0.73      0.77       354\n",
      "       promotion       0.74      0.47      0.58        68\n",
      "\n",
      "        accuracy                           0.69       876\n",
      "       macro avg       0.73      0.55      0.57       876\n",
      "    weighted avg       0.71      0.69      0.68       876\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Print the classification report for the test dataset\n",
    "print(classification_report(y_test, mnb_preds_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above it can be seen that:\n",
    "\n",
    "- There is still evidence of overfitting, as accuracy predictions using the training data are greater than those using the test data.  However, this shows better balance than under the K Nearest Neighbours classifier.\n",
    "- There is better balance between precision and recall for the training and test data.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression is a classification algorithm that employs Maximum Likelihood Estimation to generate a model capable of dividing observations into different groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Specify the classifier, in this case LogisticRegression()\n",
    "logreg = LogisticRegression(random_state=55, max_iter=15000, multi_class='multinomial')\n",
    "\n",
    "## Create a parameter grid to identify optimal parameters\n",
    "logreg_param_grid = {'C':[1,2,10],\n",
    "                     'class_weight': ['balanced', None],\n",
    "                     'solver':['newton-cg', 'sag', 'saga','lbfgs']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/isobeldaley/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/isobeldaley/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/isobeldaley/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/isobeldaley/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/isobeldaley/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/isobeldaley/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/isobeldaley/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/isobeldaley/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/isobeldaley/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/isobeldaley/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/isobeldaley/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/isobeldaley/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/isobeldaley/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Parameters</th>\n",
       "      <th>Test Score</th>\n",
       "      <th>Training Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tf_idf</td>\n",
       "      <td>{'C': 2, 'class_weight': 'balanced', 'solver': 'newton-cg'}</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>count</td>\n",
       "      <td>{'C': 1, 'class_weight': 'balanced', 'solver': 'newton-cg'}</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.71</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Dataset                                                   Parameters  \\\n",
       "0  tf_idf  {'C': 2, 'class_weight': 'balanced', 'solver': 'newton-cg'}   \n",
       "1   count  {'C': 1, 'class_weight': 'balanced', 'solver': 'newton-cg'}   \n",
       "\n",
       "   Test Score  Training Score  \n",
       "0        0.75            0.71  \n",
       "1        0.75            0.71  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model_parameters_dataset(logreg, logreg_param_grid, datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above indicates that the following combination of parameters lead to optimal performance:\n",
    "\n",
    "- **Dataset**: the data vectorised using tf-idf yields marginally better performance\n",
    "- **C**: 2\n",
    "- **class_weight**: balanced\n",
    "- **solver**: newton-cg\n",
    "\n",
    "The model is therefore fitted using these parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/isobeldaley/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=2, class_weight='balanced', dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=15000,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=213, solver='newton-cg', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Fit the model using optimal dataset and parameters\n",
    "logreg_ = LogisticRegression(random_state=213, solver='newton-cg', max_iter=15000, C=2, class_weight='balanced')\n",
    "logreg_.fit(tf_idf_X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To assess the performance of each model, predictions are generated for teh training and test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create predictions for the training and test dataset\n",
    "logreg_preds_train = logreg_.predict(tf_idf_X_train)\n",
    "logreg_preds_test = logreg_.predict(tf_idf_X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, the classification report is generated for the training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  precision    recall  f1-score   support\n",
      "\n",
      "       broadband       0.91      1.00      0.95        74\n",
      "        contract       0.89      0.95      0.92       427\n",
      "customer service       0.92      0.92      0.92       753\n",
      "          device       0.90      0.96      0.93       217\n",
      "         network       0.94      0.96      0.95       436\n",
      "           other       0.97      0.92      0.94      1341\n",
      "       promotion       0.95      0.96      0.96       253\n",
      "\n",
      "        accuracy                           0.94      3501\n",
      "       macro avg       0.93      0.95      0.94      3501\n",
      "    weighted avg       0.94      0.94      0.94      3501\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Print the classification report for the training dataset\n",
    "print(classification_report(y_train, logreg_preds_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  precision    recall  f1-score   support\n",
      "\n",
      "       broadband       0.50      0.50      0.50        10\n",
      "        contract       0.64      0.68      0.66       101\n",
      "customer service       0.75      0.72      0.74       178\n",
      "          device       0.64      0.76      0.70        59\n",
      "         network       0.82      0.79      0.80       106\n",
      "           other       0.82      0.83      0.83       354\n",
      "       promotion       0.82      0.66      0.73        68\n",
      "\n",
      "        accuracy                           0.77       876\n",
      "       macro avg       0.71      0.71      0.71       876\n",
      "    weighted avg       0.77      0.77      0.77       876\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Print the classification report for the test daaset\n",
    "print(classification_report(y_test, logreg_preds_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are similar issues with overfitting with this model.  The accuracy of predictions using the training data is about 17% higher than the model built using the test data.  \n",
    "\n",
    "**Note**: By default, an l2 penalty is included within a logistic regression model solved using 'newton-cg'.  It is unfortunately not possible to make any enhancements to this penalty in order to improve the issue of overfitting.  \n",
    "\n",
    "To see if this can be improved, an l2 penalty will be introduced.  This provides one mechanism to overcome overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can also be seen from the above that the model performs poorly when classifying tweets as 'broadband' and 'contract', but performs much better when classifying into the other categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A random forest is considered as the next possible classifier.  Random forests work by creating a number of different decision trees (specified by n_estimators) and then outputting the mode of the predictions made by each decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the classifier to be used, in this case RandomForestClassifier(), specify a random_state\n",
    "## so that the results are reproducible\n",
    "forest = RandomForestClassifier(random_state=55)\n",
    "\n",
    "## Specify the parameter grid to be assessed\n",
    "forest_param_grid = {'n_estimators': [75,150,300,450],\n",
    "                    'criterion': ['gini', 'entropy'],\n",
    "                  'max_depth':[None, 5, 10, 15],\n",
    "                  'class_weight': ['balanced', None],\n",
    "                  'bootstrap': [True, False]\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Parameters</th>\n",
       "      <th>Test Score</th>\n",
       "      <th>Training Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tf_idf</td>\n",
       "      <td>{'bootstrap': False, 'class_weight': 'balanced', 'criterion': 'gini', 'max_depth': None, 'n_estimators': 450}</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>count</td>\n",
       "      <td>{'bootstrap': False, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'n_estimators': 300}</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.69</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Dataset  \\\n",
       "0  tf_idf   \n",
       "1   count   \n",
       "\n",
       "                                                                                                      Parameters  \\\n",
       "0  {'bootstrap': False, 'class_weight': 'balanced', 'criterion': 'gini', 'max_depth': None, 'n_estimators': 450}   \n",
       "1        {'bootstrap': False, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'n_estimators': 300}   \n",
       "\n",
       "   Test Score  Training Score  \n",
       "0        0.76            0.70  \n",
       "1        0.71            0.69  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Run the best_model_parameters_dataset() function to identify the optimal dataset and parameters to use\n",
    "best_model_parameters_dataset(forest, forest_param_grid, datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen from the above, the following combinations of parameters yield optimal performance:\n",
    "- **Dataset**: Data vectorized using tf-idf vectorization\n",
    "- **bootstrap**: False\n",
    "- **class_weight**: balanced\n",
    "- **criterion**: gini\n",
    "- **max_depth**: None\n",
    "- **n_estimators**:450\n",
    "\n",
    "To investigate the performance of this model further, it is created below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=False, class_weight='balanced',\n",
       "                       criterion='gini', max_depth=None, max_features='auto',\n",
       "                       max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "                       min_impurity_split=None, min_samples_leaf=1,\n",
       "                       min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                       n_estimators=450, n_jobs=None, oob_score=False,\n",
       "                       random_state=213, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Define the random forest classifier\n",
    "forest = RandomForestClassifier(bootstrap=False, criterion='gini', max_depth=None, \n",
    "                                n_estimators=450, random_state=213, class_weight='balanced')\n",
    "\n",
    "## Fit the model to the tf_idf data\n",
    "forest.fit(tf_idf_X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate predictions using this model\n",
    "forest_preds_train = forest.predict(tf_idf_X_train)\n",
    "forest_preds_test = forest.predict(tf_idf_X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classification report is generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  precision    recall  f1-score   support\n",
      "\n",
      "       broadband       1.00      1.00      1.00        74\n",
      "        contract       0.97      1.00      0.99       427\n",
      "customer service       0.98      0.99      0.99       753\n",
      "          device       1.00      1.00      1.00       217\n",
      "         network       0.99      1.00      1.00       436\n",
      "           other       1.00      0.98      0.99      1341\n",
      "       promotion       0.99      1.00      1.00       253\n",
      "\n",
      "        accuracy                           0.99      3501\n",
      "       macro avg       0.99      1.00      0.99      3501\n",
      "    weighted avg       0.99      0.99      0.99      3501\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Generate the classification report for the training data\n",
    "print(classification_report(y_train, forest_preds_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  precision    recall  f1-score   support\n",
      "\n",
      "       broadband       0.67      0.40      0.50        10\n",
      "        contract       0.65      0.67      0.66       101\n",
      "customer service       0.73      0.71      0.72       178\n",
      "          device       0.63      0.49      0.55        59\n",
      "         network       0.81      0.78      0.80       106\n",
      "           other       0.77      0.89      0.83       354\n",
      "       promotion       0.95      0.51      0.67        68\n",
      "\n",
      "        accuracy                           0.75       876\n",
      "       macro avg       0.74      0.64      0.68       876\n",
      "    weighted avg       0.76      0.75      0.75       876\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Generate the classification report for the testing data\n",
    "print(classification_report(y_test, forest_preds_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above, there is evidence of marked overfitting (accuracy is 99% with training data, but 75% with test data).  The balance between precision and recall is mixed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the Support Vector Machine (SVM).  This model attempts to find the decision boundary which maximises the distance between the boundary and the training observatiopns.  This model includes a parameter (C), which specifies the balance between finding this optimal boundary for most datapoints, and misclassifying observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Specify the classification model, in this case a support vector machine\n",
    "svm = SVC(gamma='auto', random_state=55)\n",
    "\n",
    "## Specify the parameter grid to be used during the GridSearchCV\n",
    "svm_param_grid = {'C':[1,5,10],\n",
    "                'class_weight':['balanced', None]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Parameters</th>\n",
       "      <th>Test Score</th>\n",
       "      <th>Training Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>count</td>\n",
       "      <td>{'C': 10, 'class_weight': 'balanced'}</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tf_idf</td>\n",
       "      <td>{'C': 1, 'class_weight': None}</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.38</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Dataset                             Parameters  Test Score  Training Score\n",
       "1   count  {'C': 10, 'class_weight': 'balanced'}        0.55            0.47\n",
       "0  tf_idf         {'C': 1, 'class_weight': None}        0.40            0.38"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Run the best_model_parameters_dataset() function to identify the optimal dataset and parameters to use\n",
    "best_model_parameters_dataset(svm, svm_param_grid, datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen from the above, performance of the SVM is weak relative to some of the other models (e.g. Multinomial Logistic Regression).  For this reason, we will not progress with this classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XG Boost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The XG Boost model is not part of the sklearn library. Therefore GridSearch will not be performed for this model. However, the performance of this model will be compared both using the tf-idf dataset and the count vectorized dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XG Boost with TF-IDF Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
       "              learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
       "              min_child_weight=1, missing=None, n_estimators=100, n_jobs=1,\n",
       "              nthread=None, objective='multi:softprob', random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "              silent=None, subsample=1, verbosity=1)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Specify the classifier, in this case XG Boost\n",
    "boost = xgb.XGBClassifier()\n",
    "\n",
    "## Fit the model using the training data\n",
    "boost.fit(tf_idf_X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create predictions for the training and test datasets\n",
    "boost_preds_train = boost.predict(tf_idf_X_train)\n",
    "boost_preds_test = boost.predict(tf_idf_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7649243073407598"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Print the accuracy score for XG Boost using training data\n",
    "accuracy_score(y_train, boost_preds_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7203196347031964"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Print the accuracy score for XG Boost using test data\n",
    "accuracy_score(y_test, boost_preds_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XG Boost with Count Vectorized Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
       "              learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
       "              min_child_weight=1, missing=None, n_estimators=100, n_jobs=1,\n",
       "              nthread=None, objective='multi:softprob', random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "              silent=None, subsample=1, verbosity=1)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Specify the classifier, in this case XG Boost\n",
    "boost = xgb.XGBClassifier()\n",
    "\n",
    "## Fit the model using the training data\n",
    "boost.fit(count_X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create predictions for the training and test datasets\n",
    "boost_preds_train = boost.predict(count_X_train)\n",
    "boost_preds_test = boost.predict(count_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7223650385604113"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Print the accuracy score for XG Boost using training data\n",
    "accuracy_score(y_train, boost_preds_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7191780821917808"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Print the accuracy score for XG Boost using test data\n",
    "accuracy_score(y_test, boost_preds_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen from the above, the XG Boost performs slightly better when using the tf-idf dataset.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Unlabelled Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having built the initial model, it is possible to investigate whether predicting labels for the unlabelled data, and then  re-running this model to include the predicted categories for the unlabelled data, will improve performance.\n",
    "\n",
    "Before doing this, it is first necessary to import the unlabelled data, which has been stored in a Pandas DataFrame using pickle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import the unlabelled data using Pickle\n",
    "unlabelled_df = pd.read_pickle('cleaned_unlabelled_tweets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>network</th>\n",
       "      <th>datetime</th>\n",
       "      <th>original_tweet</th>\n",
       "      <th>subject</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>lemmatized_tweets_tokens</th>\n",
       "      <th>lemmatized_tweets_string</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>@VodafoneUK</td>\n",
       "      <td>2019-12-04 01:20:56</td>\n",
       "      <td>@avipan_lko @VodafoneIN @VodafoneGroup @VodafoneUK @TRAI @rssharma3 @rsprasad @narendramodi ऐसे ही रहेगा वोडाफोन सुधार हो ही नहीं सकता</td>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>@VodafoneUK</td>\n",
       "      <td>2019-12-04 00:48:17</td>\n",
       "      <td>@danielrome18 @VodafoneUK fucking hell 😱</td>\n",
       "      <td></td>\n",
       "      <td>-0.6</td>\n",
       "      <td>[fucking, hell]</td>\n",
       "      <td>fucking hell</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>@VodafoneUK</td>\n",
       "      <td>2019-12-03 22:46:47</td>\n",
       "      <td>@VodafoneUK I was hoping you’d say that.</td>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "      <td>[hoping, youd, say]</td>\n",
       "      <td>hoping youd say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>@VodafoneUK</td>\n",
       "      <td>2019-12-03 22:38:07</td>\n",
       "      <td>@VodafoneUK please explain https://t.co/PhHJdMbrG9</td>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "      <td>[please, explain]</td>\n",
       "      <td>please explain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>@VodafoneUK</td>\n",
       "      <td>2019-12-03 22:22:46</td>\n",
       "      <td>@Townsley85 @VodafoneUK Hear, hear!</td>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "      <td>[hear, hear]</td>\n",
       "      <td>hear hear</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        network            datetime  \\\n",
       "39  @VodafoneUK 2019-12-04 01:20:56   \n",
       "41  @VodafoneUK 2019-12-04 00:48:17   \n",
       "56  @VodafoneUK 2019-12-03 22:46:47   \n",
       "61  @VodafoneUK 2019-12-03 22:38:07   \n",
       "65  @VodafoneUK 2019-12-03 22:22:46   \n",
       "\n",
       "                                                                                                                            original_tweet  \\\n",
       "39  @avipan_lko @VodafoneIN @VodafoneGroup @VodafoneUK @TRAI @rssharma3 @rsprasad @narendramodi ऐसे ही रहेगा वोडाफोन सुधार हो ही नहीं सकता   \n",
       "41                                                                                                @danielrome18 @VodafoneUK fucking hell 😱   \n",
       "56                                                                                                @VodafoneUK I was hoping you’d say that.   \n",
       "61                                                                                      @VodafoneUK please explain https://t.co/PhHJdMbrG9   \n",
       "65                                                                                                     @Townsley85 @VodafoneUK Hear, hear!   \n",
       "\n",
       "   subject  sentiment lemmatized_tweets_tokens lemmatized_tweets_string  \n",
       "39                0.0                       []                           \n",
       "41               -0.6          [fucking, hell]             fucking hell  \n",
       "56                0.0      [hoping, youd, say]          hoping youd say  \n",
       "61                0.0        [please, explain]           please explain  \n",
       "65                0.0             [hear, hear]                hear hear  "
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Display the first five rows of the DataFrame\n",
    "unlabelled_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, it is necessary to vectorize the unlabelled tweets using the tf_idf vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Transform unlabelled tweets using the tfidf_vectorizer\n",
    "tf_idf_unlabelled_X_train = tfidf_vectorizer.transform(unlabelled_df['lemmatized_tweets_string'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once this is complete, it is possible to predict categorise for the unlabelled data using the chosen model.  The logistic regression model is chosen as it offers a good balance between accuracy for the training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make predictions for the unlabelled tweets\n",
    "unlabelled_y_train = logreg.predict(tf_idf_unlabelled_X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the unlabelled tweets and the predicted categories can be combined into a single dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a combined list of tweets (X)\n",
    "all_X_train = list(X_train)\n",
    "all_X_train.extend(unlabelled_df['lemmatized_tweets_string'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Vectorize the complete list of tweets\n",
    "tf_idf_all_X_train = tfidf_vectorizer.transform(all_X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a combined list of categories (y)\n",
    "all_y_train = np.append(values=np.array(y_train), arr=unlabelled_y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-Run Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, a new model can be created using the enlarged dataset.  To ensure assessment is comparable to prior models, the same test dataset will be used as before.  A multinomial logistic regression model will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a list containing the combined dataset\n",
    "complete_datasets = [{'name': 'complete_tf_idf','X_train': tf_idf_all_X_train, 'y_train':all_y_train, 'X_test': tf_idf_X_test, 'y_test':y_test}]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Specify the classifier, in this case LogisticRegression()\n",
    "logreg = LogisticRegression(random_state=55, max_iter=15000, multi_class='multinomial')\n",
    "\n",
    "## Create a parameter grid to identify optimal parameters\n",
    "logreg_param_grid = {'C':[1,2,10],\n",
    "                     'class_weight': ['balanced', None],\n",
    "                     'solver':['newton-cg', 'sag', 'saga','lbfgs']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/isobeldaley/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/isobeldaley/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/isobeldaley/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/isobeldaley/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/isobeldaley/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/isobeldaley/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/isobeldaley/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/isobeldaley/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/isobeldaley/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/isobeldaley/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/isobeldaley/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/isobeldaley/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/isobeldaley/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/isobeldaley/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/isobeldaley/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/isobeldaley/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/isobeldaley/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/isobeldaley/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Parameters</th>\n",
       "      <th>Test Score</th>\n",
       "      <th>Training Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>complete_tf_idf</td>\n",
       "      <td>{'C': 1, 'class_weight': None, 'solver': 'saga'}</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Dataset                                        Parameters  \\\n",
       "0  complete_tf_idf  {'C': 1, 'class_weight': None, 'solver': 'saga'}   \n",
       "\n",
       "   Test Score  Training Score  \n",
       "0        0.35            0.32  "
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Run the best_model_parameters_dataset() function to identify the optimal dataset and parameters to use\n",
    "best_model_parameters_dataset(logreg, logreg_param_grid, complete_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results above suggest that the model deteriorates when using unlabelled data categorised by the original model.  This will therefore not be taken further.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model using Pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The chosen logistic regression model (run using only the labelled data) is saved using a Pickle object.  This model will be used to generate predictions in the next step **Step 5 - Interpret Results**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model.pkl']"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Use Pickle and joblib to save the Pickle object\n",
    "joblib.dump(logreg_, 'model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(X_test, open( \"X_test.pkl\", \"wb\" ))\n",
    "pickle.dump(y_test, open( \"y_test.pkl\", \"wb\" ))\n",
    "pickle.dump(tf_idf_X_test, open(\"tf_idf_X_test.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 5016)\t0.6398374398214728\n",
      "  (0, 4986)\t0.31417185727276653\n",
      "  (0, 4478)\t0.3420347115252299\n",
      "  (0, 3851)\t0.31065161998539537\n",
      "  (0, 3826)\t0.39355013074044704\n",
      "  (0, 3418)\t0.2754627333004195\n",
      "  (0, 2412)\t0.21828994243809113\n"
     ]
    }
   ],
   "source": [
    "print(tf_idf_X_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
